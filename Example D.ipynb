{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "379df403",
   "metadata": {},
   "source": [
    "# Project enda : Example D\n",
    "\n",
    "\n",
    "Install all the required packages in your python virtualenv:\n",
    "```bash\n",
    "pip install numexpr bottleneck pandas enda jupyter h2o scikit-learn statsmodels matplotlib joblib\n",
    "# more packages used for feature engineering below\n",
    "pip install jours-feries-france vacances-scolaires-france Unidecode \n",
    "```\n",
    "\n",
    "In this example we will set up a simple dayahead energy production prediction.  \n",
    "\n",
    "We set ourselves in a setup as if we were **exactly on 2021-01-01**. We want to predict the production of several power plants for the next day on 30 min time-step intervals. In this example, we have exactly one year of data that spans the whole year 2020. This data is split in several files that are likely to be the ones obtained from a typical ETL processing. We always separate the data between the type of power plant (solar, wind, run of river), because they have very different behaviour, as we'll see. We thus consider:\n",
    "\n",
    "- a list of stations with their associated installed_capacity (`wind_stations.csv`, `historic_solar.csv`, `historic_river.csv`). These files summarize contracts we may have with the aforementioned power stations. \n",
    "\n",
    "- the historical power generation data for the power stations along the year 2020. Note this has to be obtained by yourself according to your needs (as an example, Enercoop data are regularly published from the French TSO)\n",
    "\n",
    "- the historical meteo data for the power stations along the year 2020. This also has to be obtained on your side (as an example, we regularly gather meteo information from GFS using the zipcode of the power plants in our portfolio).\n",
    "\n",
    "- a list of events (planned shutdowns, outages) that may have dirupted the usual installed capacity of the power stations. \n",
    "\n",
    "\n",
    "We will : \n",
    "- set up the relevant training datasets and do some feature engineering ;\n",
    "- set up several models of training ;\n",
    "- predict the dayahead energy production per power plant, and its aggregated counterpart. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448d14c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import enda\n",
    "import datetime\n",
    "import numpy as np\n",
    "from pandas.api.types import is_string_dtype\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_colwidth = 30\n",
    "enda.__file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264a1aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = '/Users/clement.jeannesson/Jobs/enda/'\n",
    "DIR_TEST = '/Users/clement.jeannesson/Jobs/enda/tests/example_d/input_example/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cd614a",
   "metadata": {},
   "source": [
    "## Portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93639f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The details of power stations can be interpreted as enda contracts. \n",
    "stations_wind =  enda.Contracts.read_contracts_from_file(os.path.join(DIR_TEST, \"wind_stations.csv\"))\n",
    "stations_solar = enda.Contracts.read_contracts_from_file(os.path.join(DIR_TEST, \"solar_stations.csv\"))\n",
    "stations_river = enda.Contracts.read_contracts_from_file(os.path.join(DIR_TEST, \"river_stations.csv\"))\n",
    "stations=[stations_wind, stations_solar, stations_river]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b25535",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = [display(station) for station in stations]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8daed03e",
   "metadata": {},
   "source": [
    "For this example, we have chosen to consider four power stations of each type. \n",
    "\n",
    "Exactly as contracts data, we have a date start and a date of end, and some characteristics that stay valid over that time lap. The end date is properly set in most cases. This differs from consumption contracts (cf. Example A), for which no end date are provided.  \n",
    "\n",
    "We want to get a daily data for these stations. Here, a change of installed capacity is spotted for ```eo_4```. This has to be taken care of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0239e619",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# all enda functions in this notebook example are defined through wrappers\n",
    "\n",
    "def get_stations_daily(df):\n",
    "    return enda.PowerStations.get_stations_daily(\n",
    "               df, \n",
    "               station_col='station',\n",
    "               date_start_col=\"date_start\",\n",
    "               date_end_exclusive_col=\"date_end_exclusive\"\n",
    "           )\n",
    "\n",
    "stations_daily = [get_stations_daily(station) for station in stations]\n",
    "display(stations_daily[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c0c238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the portfolio between dates\n",
    "\n",
    "def get_stations_between_dates(df):\n",
    "    return enda.PowerStations.get_stations_between_dates(\n",
    "               df, \n",
    "               start_datetime = pd.to_datetime('2020-01-01'),\n",
    "               end_datetime_exclusive = pd.to_datetime('2021-01-01')\n",
    "           )\n",
    "\n",
    "stations_daily = [get_stations_between_dates(station) for station in stations_daily]\n",
    "display(stations_daily[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be091a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check it's ok for wind station \n",
    "stations_wind_daily = stations_daily[0]\n",
    "dates_test = [datetime.datetime(2020, 2, 17) + datetime.timedelta(days=x) for x in range(4)]\n",
    "stations_wind_daily[(stations_wind_daily.index.get_level_values(\"station\") == \"eo_4\") &\n",
    "                    (stations_wind_daily.index.get_level_values(\"date\").isin(dates_test))\n",
    "                   ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6a52ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# interpolate data to a sub-daily scale using enda.\n",
    "\n",
    "def interpolate_stations_to_subdaily_data(df):\n",
    "    return enda.TimeSeries.interpolate_daily_to_sub_daily_data(\n",
    "               df,\n",
    "               freq='30min', \n",
    "               tz='Europe/Paris', \n",
    "               index_name='time'\n",
    "           )\n",
    "\n",
    "stations_finergrid = [interpolate_stations_to_subdaily_data(station) for station in stations_daily]\n",
    "display(stations_finergrid[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c51b704",
   "metadata": {},
   "source": [
    "At this point we have the portfolio information per day. It is useful to take into account the outages and shutdowns.\n",
    "\n",
    "### Take into account outages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143f3b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read outages \n",
    "filepath =  os.path.join(DIR_TEST, \"events.csv\")\n",
    "\n",
    "outages = enda.PowerStations.read_outages_from_file(\n",
    "    filepath, \n",
    "    station_col='station',\n",
    "    time_start_col=\"time_start\", \n",
    "    time_end_exclusive_col=\"time_end\", \n",
    "    pct_outages_col=\"impact_production_pct_kw\",\n",
    "    tzinfo=\"Europe/Paris\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeaadeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(outages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5528523c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrapper to integrate the outages to the stations portfolio.\n",
    "def integrate_outages_to_stations(df):\n",
    "    return enda.PowerStations.integrate_outages(\n",
    "        stations=df,   \n",
    "        outages=outages, \n",
    "        station_col=\"station\",\n",
    "        time_start_col=\"time_start\",\n",
    "        time_end_exclusive_col=\"time_end\", \n",
    "        installed_capacity_col=\"installed_capacity_kw\",\n",
    "        pct_outages_col=\"impact_production_pct_kw\"\n",
    "    )\n",
    "\n",
    "stations_historic = [integrate_outages_to_stations(station) for station in stations_finergrid]\n",
    "display(stations_historic[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93588f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the outages have been corectly taken into account \n",
    "stations_wind_daily = stations_historic[0]\n",
    "stations_wind_daily.loc[(stations_wind_daily.index.get_level_values(\"station\") == \"eo_2\")\n",
    "                    & (stations_wind_daily.index.get_level_values(\"time\") >= pd.to_datetime('2020-02-23 22:00:00+01:00'))\n",
    "                    & (stations_wind_daily.index.get_level_values(\"time\") < pd.to_datetime('2020-02-24 02:00:00+01:00'))\n",
    "                   ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb00b80",
   "metadata": {},
   "source": [
    "# Meteo and production\n",
    "\n",
    "Here, the meteo and production data have been retrived over the year 2020.  \n",
    "We need to set them on the frequency of interest, which is a 30 minutes time lap. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3b2b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve meteo information. We only have it for solar and wind stations\n",
    "meteo_wind = pd.read_csv(os.path.join(DIR_TEST, \"historic_meteo_wind.csv\"))\n",
    "meteo_solar = pd.read_csv(os.path.join(DIR_TEST, \"historic_meteo_solar.csv\"))\n",
    "\n",
    "for df in [meteo_wind, meteo_solar]:\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    df['time'] = enda.TimeSeries.align_timezone(df['time'], tzinfo = 'Europe/Paris')\n",
    "    df.set_index([\"station\", \"time\"], inplace=True)\n",
    "    \n",
    "meteo = [meteo_wind, meteo_solar]\n",
    "_ = [display(_) for _ in meteo]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e62af1",
   "metadata": {},
   "source": [
    "Please note meteo data cannot be separated from the power plant, because of the meteo is related specific location of each plant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3adf940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpolate the meteo information to a 30 minutes scale\n",
    "def apply_freq_to_sub_freq(df):\n",
    "    return enda.TimeSeries.interpolate_freq_to_sub_freq_data(\n",
    "               df,\n",
    "               freq='30min', \n",
    "               tz='Europe/Paris',\n",
    "               index_name='time',\n",
    "               method=\"linear\"\n",
    "           )\n",
    "\n",
    "meteo_wind, meteo_solar = (apply_freq_to_sub_freq(m) for m in meteo[0:2])\n",
    "meteo_historic = [meteo_wind, meteo_solar]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca10418",
   "metadata": {},
   "outputs": [],
   "source": [
    "meteo_wind"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e90efc8",
   "metadata": {},
   "source": [
    "Get production information. This information is avaliable from the TSO. It is provided on a fine 10-minutes timestep, and we need to average it over the half-hour scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ace0c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve production information.\n",
    "production_wind = pd.read_csv(os.path.join(DIR_TEST, \"historic_production_wind.csv\"))\n",
    "production_solar = pd.read_csv(os.path.join(DIR_TEST, \"historic_production_solar.csv\"))\n",
    "production_river = pd.read_csv(os.path.join(DIR_TEST, \"historic_production_river.csv\"))\n",
    "\n",
    "for df in [production_wind, production_solar, production_river]:\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    df['time'] = enda.TimeSeries.align_timezone(df['time'], tzinfo = 'Europe/Paris')\n",
    "    df.set_index([\"station\", \"time\"], inplace=True)\n",
    "\n",
    "production = [production_wind, production_solar, production_river]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3016a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us display production for run of river stations\n",
    "display(production_river)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de5059f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us average the production over a 30 minutes time scale.\n",
    "def apply_average_to_upper_freq(df):\n",
    "    return enda.TimeSeries.average_to_upper_freq(\n",
    "               df,\n",
    "               freq='30min', \n",
    "               tz='Europe/Paris',\n",
    "               index_name='time'\n",
    "           )\n",
    "\n",
    "production_wind, production_solar, production_river = (apply_average_to_upper_freq(prod) for prod in production)\n",
    "production_historic = [production_wind, production_solar, production_river]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b3b8ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(production_river)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db91d7c3",
   "metadata": {},
   "source": [
    "We gathered information about the power stations in our example portfolio over the year 2020, as well as production data, and meteo information (for solar and wind only). We managed to set them on a 30-minutes scale. We need to merge these data together, to produce training sets that will be useful for our prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e4f85e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# wrapper to merge the portfolio, production, and meteo dataframes for each type \n",
    "# of power plant.\n",
    "def merge_stations_and_features(df1, df2):\n",
    "    df = pd.merge(df1, df2, how='left', left_index=True, right_index=True)\n",
    "    return df.dropna()\n",
    "    \n",
    "full_historic = [merge_stations_and_features(station, production)\n",
    "                 for station, production in zip(stations_historic, production_historic)]\n",
    "\n",
    "full_historic[0:2] = [merge_stations_and_features(station, meteo)\n",
    "                      for station, meteo in zip(full_historic[0:2], meteo_historic)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ab621d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us display all the dataframes\n",
    "_ = [display(historic) for historic in full_historic]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d910b948",
   "metadata": {},
   "source": [
    "## Featurize\n",
    "\n",
    "Let's try to add some feature for the solar information, namely the cos and sin of dates, as what has been done in the Example 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23df0d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# using enda.feature_engineering.calendar for solar\n",
    "full_historic[1] = enda.DatetimeFeature.split_datetime(\n",
    "        full_historic[1], split_list = ['minuteofday', 'dayofyear']\n",
    "    )\n",
    "\n",
    "full_historic[1] = enda.DatetimeFeature.encode_cyclic_datetime_index(\n",
    "        full_historic[1], split_list = ['minuteofday', 'dayofyear']\n",
    "    )\n",
    "\n",
    "display(full_historic[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84576d11",
   "metadata": {},
   "source": [
    "## Compute the load_factor\n",
    "\n",
    "The *load factor* is the target of the algorithm. \n",
    "Let's compute it from the `installed_capacity` and the `power_kw` fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e501098",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# wrapper to compute the load factor. \n",
    "# We drop the power_kw information during that step.\n",
    "def compute_load_factor(df):\n",
    "    return enda.PowerStations.compute_load_factor(\n",
    "               df, \n",
    "               installed_capacity_kw='installed_capacity_kw', \n",
    "               power_kw='power_kw',\n",
    "               drop_power_kw=True\n",
    "           )           \n",
    "\n",
    "train_data = (compute_load_factor(historic) for historic in full_historic)\n",
    "train_wind, train_solar, train_river = train_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382772d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(train_wind, train_solar, train_river)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17ed6e2",
   "metadata": {},
   "source": [
    "We have here the training datasets we'll use to make our prediction, which have been built using the enda utilities function, and some historical information gathered from the TSO, diverse meteo information suppliers, and contracts data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5bb30f",
   "metadata": {},
   "source": [
    "# Make a prediction\n",
    "\n",
    "Let's use the enda algorithms to make a simple power prediction. \n",
    "\n",
    "First of all, let us retrieve the forecast dataframes used for our prediction. They contain portfolio and meteo information for the 02/02/2021. They actually have been processed following a similar pipeline than the the one presented in this notebook for the training dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fded32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retreive forecast data\n",
    "forecast_wind = pd.read_csv(os.path.join(DIR_TEST,  \"forecast_wind.csv\"))\n",
    "forecast_solar = pd.read_csv(os.path.join(DIR_TEST, \"forecast_solar.csv\"))\n",
    "forecast_river = pd.read_csv(os.path.join(DIR_TEST, \"forecast_river.csv\"))\n",
    "\n",
    "for df in [forecast_wind, forecast_solar, forecast_river]:\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    df['time'] = enda.TimeSeries.align_timezone(df['time'], tzinfo = 'Europe/Paris')\n",
    "    df.set_index([\"station\", \"time\"], inplace=True)\n",
    "\n",
    "forecast = [forecast_wind, forecast_solar, forecast_river]\n",
    "display(forecast_wind, forecast_solar, forecast_river)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fc618b",
   "metadata": {},
   "source": [
    "We need to import the ML backends from enda, as well as the anda wrapper meant to handle calculations specific to the power prediction, ie. objects of the class PowerPredictor. This class enables to use EndaEstimators for all power plants considered as individual plants, or instances of a theoretical standard one. That latter approach is the standard power plant method, in which all observations are merged together to form a training dataset, and individual prperties of each plant are considered as additional features of the algorithm (this is notably the case of the `installed_capacity`column in the present example.\n",
    "\n",
    "Here, we will use linear predictors coupled with a standard power plant approach for the solar and wind stations. For the hydraulic plants, the chosen methodology will be slightly different, as we use in practice a much more naive technique, which is simply a recopy of the last observation for each power plant. This means using a non standard power plant approach coupled with the so-called `Recopy()` enda estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251a394b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ML backends\n",
    "from enda.ml_backends.sklearn_estimator import EndaSklearnEstimator\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from enda.estimators import EndaEstimatorRecopy\n",
    "\n",
    "# import power predictors\n",
    "from enda.power_predictor import PowerPredictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd864d9",
   "metadata": {},
   "source": [
    "### Run of river prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d05f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a PowerPredictor obejct\n",
    "river_predictor = PowerPredictor(standard_plant=False)\n",
    "\n",
    "# use PowerPredictor to train the estimator from the run of river data, \n",
    "# and from a naive recopy estimator\n",
    "river_predictor.train(train_river, estimator=EndaEstimatorRecopy(period='1D'), target_col=\"load_factor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05eda42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see the guts of what's happening inside: as standard plant is False, a single estimator is created \n",
    "# for each power plant.\n",
    "# It is trained individually on the available data; here, we need to naively recopy the data. \n",
    "# The prod_estimators field of the instance of PowerPredictor is a dictionary with the station ID, \n",
    "# and the estimator that we can train. Here we can access the fields training_data specific to \n",
    "# EndaEstimatorRecopy()\n",
    "_ = [display(x[0], pd.DataFrame(x[1].training_data.T)) for x in river_predictor.prod_estimators.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d49ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once it has been trained, we can predict the power for each power plant individually, calling predict()\n",
    "# from PowerPredictor()\n",
    "pred_river = river_predictor.predict(forecast_river, target_col=\"load_factor\")\n",
    "pred_river"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce510c5d",
   "metadata": {},
   "source": [
    "### Wind prediction\n",
    "\n",
    "For the wind prediction, we will use a Sklearn Linear predictor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83353937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a PowerPredictor object\n",
    "wind_predictor = PowerPredictor(standard_plant=True)\n",
    "\n",
    "# use a SkLearn Linear Regression estimator\n",
    "lin_reg = EndaSklearnEstimator(LinearRegression())\n",
    "\n",
    "# train the estimator\n",
    "wind_predictor.train(train_wind, estimator=lin_reg, target_col=\"load_factor\")\n",
    "\n",
    "# predict\n",
    "pred_wind = wind_predictor.predict(forecast_wind, target_col=\"load_factor\")\n",
    "pred_wind"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9956d52",
   "metadata": {},
   "source": [
    "### Solar prediction\n",
    "\n",
    "for the solar prediction, we can use a linear regression model, or even a naive recopy model, still using a standard power plant approach. Note that this is only made for the purpose of this example, because such a naive estimator is the simplest one we can think of. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c273b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a PowerPredictor object\n",
    "solar_predictor = PowerPredictor(standard_plant=True)\n",
    "\n",
    "# train the estimator\n",
    "solar_predictor.train(train_solar, estimator=EndaEstimatorRecopy(period='1D'), target_col=\"load_factor\")\n",
    "\n",
    "# predict\n",
    "pred_solar = solar_predictor.predict(forecast_solar, target_col=\"load_factor\")\n",
    "pred_solar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b331f61",
   "metadata": {},
   "source": [
    "## Getting back to power prediction\n",
    "\n",
    "To get back to power prediction, we simply need to use the installed capacity field and multiply it by the load factor to find again the power (kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa697a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we start by merging again the installed_capacity (kw) field\n",
    "prediction_river = merge_stations_and_features(forecast_river.loc[:, [\"installed_capacity_kw\"]], pred_river)\n",
    "prediction_wind = merge_stations_and_features(forecast_wind.loc[:, [\"installed_capacity_kw\"]], pred_wind)\n",
    "prediction_solar = merge_stations_and_features(forecast_solar.loc[:, [\"installed_capacity_kw\"]], pred_solar)\n",
    "\n",
    "prediction_load_factor = [prediction_river, prediction_solar, prediction_wind]\n",
    "\n",
    "prediction_wind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c647ccc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrapper around compute_power_kw_from_load_factor\n",
    "# We drop the load_factor information during that step.\n",
    "def compute_power_kw_wrapper(df):\n",
    "    return enda.PowerStations.compute_power_kw_load_factor(\n",
    "               df, \n",
    "               installed_capacity_kw='installed_capacity_kw', \n",
    "               load_factor='load_factor',\n",
    "               drop_load_factor=True\n",
    "           )           \n",
    "\n",
    "predicted_data = (compute_power_kw_wrapper(pred) for pred in prediction_load_factor)\n",
    "predicted_wind, predicted_solar, predicted_river = predicted_data\n",
    "predicted_river"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f894d3ba",
   "metadata": {},
   "source": [
    "## Conclusion \n",
    "\n",
    "We have been able to build a simple prediction using (or not) a standard power plant approach for a portfolio of plants of different types. It is possible to go further, notably performing a backtesting to explore the performance of the algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e458a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
